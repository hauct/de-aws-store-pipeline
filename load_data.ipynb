{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from credentials.db_info import DB_INFO\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_parquet_to_df(file_path):\n",
    "    # Đọc file parquet và chuyển thành pandas dataframe\n",
    "    df = pd.read_parquet(file_path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to insert data into the 'dim_product' table\n",
    "def insert_data_to_product_table(df, DB_INFO):\n",
    "    # Select necessary columns from the DataFrame\n",
    "    adjusted_df = df[['product_id', 'category', 'sub_category', 'product_name', 'buying_price', 'selling_price']].drop_duplicates(subset=['product_id'])\n",
    "\n",
    "    # Create a connection to the database\n",
    "    engine = create_engine(f\"postgresql://{DB_INFO['DB_USER']}:{DB_INFO['DB_PASSWORD']}@{DB_INFO['DB_HOST']}/{DB_INFO['DB_DATABASE']}\")\n",
    "\n",
    "    # Create an SQL statement to create the 'dim_product' table with necessary columns and data types\n",
    "    create_table_query = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {DB_INFO['DB_SCHEMA']}.dim_product (\n",
    "        product_id VARCHAR PRIMARY KEY,\n",
    "        category VARCHAR,\n",
    "        sub_category VARCHAR,\n",
    "        product_name VARCHAR\n",
    "    )\n",
    "    \"\"\"\n",
    "    # Execute the create table query\n",
    "    with engine.begin() as transaction:\n",
    "        transaction.execute(create_table_query)\n",
    "\n",
    "    # Insert data from the DataFrame into the 'dim_product' table, with 'product_id' as the primary key\n",
    "    for index, row in adjusted_df.iterrows():\n",
    "        # Replace single quotes with two single quotes in the product_name value\n",
    "        product_name = row['product_name'].replace(\"'\", \"\").replace(\"%\",\"percent\")\n",
    "        insert_query = f\"\"\"\n",
    "        INSERT INTO {DB_INFO['DB_SCHEMA']}.dim_product (product_id, category, sub_category, product_name)\n",
    "        VALUES ('{row['product_id']}', '{row['category']}', '{row['sub_category']}', '{product_name}')\n",
    "        ON CONFLICT (product_id) DO NOTHING\n",
    "        \"\"\"\n",
    "        # Execute the insert query\n",
    "        with engine.begin() as transaction:\n",
    "            transaction.execute(insert_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to insert data into the 'dim_customer' table\n",
    "def insert_data_to_customer_table(df, DB_INFO):\n",
    "    # Select necessary columns from the DataFrame\n",
    "    adjusted_df = df[['customer_id', 'customer_name', 'birth_date', 'phone_number']].drop_duplicates(subset=['customer_id'])\n",
    "\n",
    "    # Create a connection to the database\n",
    "    engine = create_engine(f\"postgresql://{DB_INFO['DB_USER']}:{DB_INFO['DB_PASSWORD']}@{DB_INFO['DB_HOST']}/{DB_INFO['DB_DATABASE']}\")\n",
    "\n",
    "    # Create an SQL statement to create the 'dim_customer' table with necessary columns and data types\n",
    "    create_table_query = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {DB_INFO['DB_SCHEMA']}.dim_customer (\n",
    "        customer_id VARCHAR PRIMARY KEY,\n",
    "        customer_name VARCHAR,\n",
    "        birth_date DATE,\n",
    "        phone_number INTEGER\n",
    "    )\n",
    "    \"\"\"\n",
    "    # Execute the create table query\n",
    "    with engine.begin() as transaction:\n",
    "        transaction.execute(create_table_query)\n",
    "\n",
    "    # Insert data from the DataFrame into the 'dim_customer' table, with 'customer_id' as the primary key\n",
    "    for index, row in adjusted_df.iterrows():\n",
    "        insert_query = f\"\"\"\n",
    "        INSERT INTO {DB_INFO['DB_SCHEMA']}.dim_customer (customer_id, customer_name, birth_date, phone_number)\n",
    "        VALUES ('{row['customer_id']}', '{row['customer_name']}', '{row['birth_date']}', '{row['phone_number']}')\n",
    "        ON CONFLICT (customer_id) DO NOTHING\n",
    "        \"\"\"\n",
    "        # Execute the insert query\n",
    "        with engine.begin() as transaction:\n",
    "            transaction.execute(insert_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_data_to_address_table(df, DB_INFO):\n",
    "    # Select necessary columns from the DataFrame\n",
    "    adjusted_df = df[['address_id', 'province', 'district', 'ward', 'ship_cost']].drop_duplicates(subset=['address_id'])\n",
    "\n",
    "    # Create a connection to the database\n",
    "    engine = create_engine(f\"postgresql://{DB_INFO['DB_USER']}:{DB_INFO['DB_PASSWORD']}@{DB_INFO['DB_HOST']}/{DB_INFO['DB_DATABASE']}\")\n",
    "\n",
    "    # Create an SQL statement to create the 'dim_address' table with necessary columns and data types\n",
    "    create_table_query = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {DB_INFO['DB_SCHEMA']}.dim_address (\n",
    "        address_id VARCHAR PRIMARY KEY,\n",
    "        province VARCHAR,\n",
    "        district VARCHAR,\n",
    "        ward VARCHAR,\n",
    "        ship_cost INTEGER\n",
    "    )\n",
    "    \"\"\"\n",
    "    # Execute the create table query\n",
    "    with engine.begin() as transaction:\n",
    "        transaction.execute(create_table_query)\n",
    "\n",
    "    # Insert data from the DataFrame into the 'dim_address' table, with 'address_id' as the primary key\n",
    "    for index, row in adjusted_df.iterrows():\n",
    "        # Replace single quotes with double quotes\n",
    "        province = row['province'].replace(\"'\", \"\")\n",
    "        district = row['district'].replace(\"'\", \"\") if row['district'] is not None else row['district']\n",
    "        ward = row['ward'].replace(\"'\", \"\") if row['ward'] is not None else row['ward']\n",
    "\n",
    "        insert_query = f\"\"\"\n",
    "        INSERT INTO {DB_INFO['DB_SCHEMA']}.dim_address (address_id, province, district, ward, ship_cost)\n",
    "        VALUES ('{row['address_id']}', '{province}', '{district}', '{ward}', '{row['ship_cost']}')\n",
    "        ON CONFLICT (address_id) DO NOTHING\n",
    "        \"\"\"\n",
    "        # Execute the insert query\n",
    "        with engine.begin() as transaction:\n",
    "            transaction.execute(insert_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_data_to_order_table(df, DB_INFO):\n",
    "    # Select necessary columns from the DataFrame\n",
    "    adjusted_df = df[['order_id', 'order_date', 'ship_date', 'customer_id', 'address_id', 'product_id', 'product_number', 'revenue', 'cost', 'discount', 'profit']].drop_duplicates(subset=['order_id'])\n",
    "\n",
    "    # Create a connection to the database\n",
    "    engine = create_engine(f\"postgresql://{DB_INFO['DB_USER']}:{DB_INFO['DB_PASSWORD']}@{DB_INFO['DB_HOST']}/{DB_INFO['DB_DATABASE']}\")\n",
    "\n",
    "    # Create an SQL statement to create the 'dim_order' table with necessary columns and data types\n",
    "    create_table_query = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {DB_INFO['DB_SCHEMA']}.dim_order (\n",
    "    order_id VARCHAR PRIMARY KEY,\n",
    "    order_date DATE,\n",
    "    ship_date DATE,\n",
    "    customer_id VARCHAR,\n",
    "    address_id VARCHAR,\n",
    "    product_id VARCHAR,\n",
    "    product_number INTEGER,\n",
    "    revenue FLOAT,\n",
    "    cost FLOAT,\n",
    "    discount FLOAT,\n",
    "    profit FLOAT,\n",
    "    FOREIGN KEY (customer_id) REFERENCES {DB_INFO['DB_SCHEMA']}.dim_customer(customer_id),\n",
    "    FOREIGN KEY (address_id) REFERENCES {DB_INFO['DB_SCHEMA']}.dim_address(address_id),\n",
    "    FOREIGN KEY (product_id) REFERENCES {DB_INFO['DB_SCHEMA']}.dim_product(product_id)\n",
    ")\n",
    "    \"\"\"\n",
    "    # Execute the create table query\n",
    "    with engine.begin() as transaction:\n",
    "        transaction.execute(create_table_query)\n",
    "\n",
    "    # Insert data from the DataFrame into the 'dim_order' table, with 'order_id' as the primary key\n",
    "    for index, row in adjusted_df.iterrows():\n",
    "        insert_query = f\"\"\"\n",
    "        INSERT INTO {DB_INFO['DB_SCHEMA']}.dim_order (order_id, order_date, ship_date, customer_id, address_id, product_id, product_number, revenue, cost, discount, profit)\n",
    "        VALUES ('{row['order_id']}', '{row['order_date']}', '{row['ship_date']}', '{row['customer_id']}', '{row['address_id']}', '{row['product_id']}', '{row['product_number']}', '{row['revenue']}', '{row['cost']}', '{row['discount']}', '{row['profit']}')\n",
    "        ON CONFLICT (order_id) DO NOTHING\n",
    "        \"\"\"\n",
    "        # Execute the insert query\n",
    "        with engine.begin() as transaction:\n",
    "            transaction.execute(insert_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def list_dates(start, end):\n",
    "    start_date = datetime.strptime(start, \"%Y-%m-%d\")\n",
    "    end_date = datetime.strptime(end, \"%Y-%m-%d\")\n",
    "    delta = timedelta(days=1)\n",
    "    current_date = start_date\n",
    "    dates = []\n",
    "    while current_date <= end_date:\n",
    "        date_str = current_date.strftime(\"%Y-%m-%d\")\n",
    "        dates.append(f'log_{date_str}.parquet')\n",
    "        current_date += delta\n",
    "    return dates\n",
    "\n",
    "file_path_list = list_dates('2024-04-01', '2024-04-23')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAP14761-local\\AppData\\Local\\Temp\\ipykernel_12872\\2875609683.py:20: RemovedIn20Warning: Deprecated API features detected! These feature(s) are not compatible with SQLAlchemy 2.0. To prevent incompatible upgrades prior to updating applications, ensure requirements files are pinned to \"sqlalchemy<2.0\". Set environment variable SQLALCHEMY_WARN_20=1 to show all deprecation warnings.  Set environment variable SQLALCHEMY_SILENCE_UBER_WARNING=1 to silence this message. (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
      "  transaction.execute(create_table_query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingested data from log_2024-04-01.parquet to DB successfully\n",
      "Ingested data from log_2024-04-02.parquet to DB successfully\n",
      "Ingested data from log_2024-04-03.parquet to DB successfully\n",
      "Ingested data from log_2024-04-04.parquet to DB successfully\n",
      "Ingested data from log_2024-04-05.parquet to DB successfully\n",
      "Ingested data from log_2024-04-06.parquet to DB successfully\n",
      "Ingested data from log_2024-04-07.parquet to DB successfully\n",
      "Ingested data from log_2024-04-08.parquet to DB successfully\n",
      "Ingested data from log_2024-04-09.parquet to DB successfully\n",
      "Ingested data from log_2024-04-10.parquet to DB successfully\n",
      "Ingested data from log_2024-04-11.parquet to DB successfully\n",
      "Ingested data from log_2024-04-12.parquet to DB successfully\n",
      "Ingested data from log_2024-04-13.parquet to DB successfully\n",
      "Ingested data from log_2024-04-14.parquet to DB successfully\n",
      "Ingested data from log_2024-04-15.parquet to DB successfully\n",
      "Ingested data from log_2024-04-16.parquet to DB successfully\n",
      "Ingested data from log_2024-04-17.parquet to DB successfully\n",
      "Ingested data from log_2024-04-18.parquet to DB successfully\n",
      "Ingested data from log_2024-04-19.parquet to DB successfully\n",
      "Ingested data from log_2024-04-20.parquet to DB successfully\n",
      "Ingested data from log_2024-04-21.parquet to DB successfully\n",
      "Ingested data from log_2024-04-22.parquet to DB successfully\n",
      "Ingested data from log_2024-04-23.parquet to DB successfully\n"
     ]
    }
   ],
   "source": [
    "for file_path in file_path_list:            \n",
    "    # Read the parquet file and convert it to a DataFrame\n",
    "    df = read_parquet_to_df(file_path)\n",
    "    # Insert data from the DataFrame into the 'dim_product' table\n",
    "    insert_data_to_product_table(df, DB_INFO)\n",
    "    # Insert data from the DataFrame into the 'dim_customer' table\n",
    "    insert_data_to_customer_table(df, DB_INFO)\n",
    "    # Insert data from the DataFrame into the 'dim_address' table\n",
    "    insert_data_to_address_table(df, DB_INFO)\n",
    "    # Insert data from the DataFrame into the 'dim_order' table\n",
    "    insert_data_to_order_table(df, DB_INFO)\n",
    "    \n",
    "    print(f'Ingested data from {file_path.split(\"/\")[-1]} to DB successfully')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
